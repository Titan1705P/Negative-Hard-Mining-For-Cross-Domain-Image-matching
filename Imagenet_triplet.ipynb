{"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport torch\nimport torchvision\nfrom PIL import Image\nimport os.path\nimport torch.optim as optim\nimport torch.nn as nn\n\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom matplotlib import pyplot as plt\nimport torch.optim as optims\nimport random\nimport torchvision.models as models","metadata":{"id":"tx9v5kCPU-ru","execution":{"iopub.status.busy":"2024-05-02T11:40:10.829690Z","iopub.execute_input":"2024-05-02T11:40:10.830285Z","iopub.status.idle":"2024-05-02T11:40:18.488912Z","shell.execute_reply.started":"2024-05-02T11:40:10.830255Z","shell.execute_reply":"2024-05-02T11:40:18.487921Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"3ry0FF2FV1qT","outputId":"9fa285d3-009f-4ba1-8f71-df665794e819","execution":{"iopub.status.busy":"2024-05-02T11:40:23.715427Z","iopub.execute_input":"2024-05-02T11:40:23.716244Z","iopub.status.idle":"2024-05-02T11:40:23.723759Z","shell.execute_reply.started":"2024-05-02T11:40:23.716209Z","shell.execute_reply":"2024-05-02T11:40:23.722603Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"dataset_path = \"ImageNette\"\ndataset_exists = os.path.exists(dataset_path)","metadata":{"id":"0Ri_UsBhq2VY","execution":{"iopub.status.busy":"2024-05-02T11:40:31.347493Z","iopub.execute_input":"2024-05-02T11:40:31.348207Z","iopub.status.idle":"2024-05-02T11:40:31.353858Z","shell.execute_reply.started":"2024-05-02T11:40:31.348168Z","shell.execute_reply":"2024-05-02T11:40:31.352392Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%pip install --upgrade torchvision\n%pip install --upgrade matplotlib","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:40:35.516671Z","iopub.execute_input":"2024-05-02T11:40:35.517058Z","iopub.status.idle":"2024-05-02T11:43:31.115956Z","shell.execute_reply.started":"2024-05-02T11:40:35.517028Z","shell.execute_reply":"2024-05-02T11:43:31.114392Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2+cpu)\nCollecting torchvision\n  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nCollecting torch==2.3.0 (from torchvision)\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2024.2.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->torchvision)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->torchvision)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->torchvision)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->torchvision)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0->torchvision)\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\nDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2+cpu\n    Uninstalling torch-2.1.2+cpu:\n      Successfully uninstalled torch-2.1.2+cpu\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.16.2+cpu\n    Uninstalling torchvision-0.16.2+cpu:\n      Successfully uninstalled torchvision-0.16.2+cpu\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.3.0 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchvision-0.18.0 triton-2.3.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nCollecting matplotlib\n  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nDownloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: matplotlib\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.6 which is incompatible.\nfastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed matplotlib-3.8.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"trans = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n              ])","metadata":{"execution":{"iopub.status.busy":"2024-05-02T11:43:31.129242Z","iopub.execute_input":"2024-05-02T11:43:31.129678Z","iopub.status.idle":"2024-05-02T11:43:31.139649Z","shell.execute_reply.started":"2024-05-02T11:43:31.129641Z","shell.execute_reply":"2024-05-02T11:43:31.138867Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"imagenet_data = torchvision.datasets.Imagenette(\"ImageNette\", download = not dataset_exists, transform = trans)\nclass_to_indices = list(set(imagenet_data.class_to_idx.values()))","metadata":{"id":"g345Nm6nTfbz","execution":{"iopub.status.busy":"2024-04-09T14:35:51.521101Z","iopub.execute_input":"2024-04-09T14:35:51.521470Z","iopub.status.idle":"2024-04-09T14:35:51.568784Z","shell.execute_reply.started":"2024-04-09T14:35:51.521442Z","shell.execute_reply":"2024-04-09T14:35:51.567997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_to_indices","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:35:52.492692Z","iopub.execute_input":"2024-04-09T14:35:52.493413Z","iopub.status.idle":"2024-04-09T14:35:52.499290Z","shell.execute_reply.started":"2024-04-09T14:35:52.493380Z","shell.execute_reply":"2024-04-09T14:35:52.498350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:35:53.255142Z","iopub.execute_input":"2024-04-09T14:35:53.255512Z","iopub.status.idle":"2024-04-09T14:35:53.260056Z","shell.execute_reply.started":"2024-04-09T14:35:53.255483Z","shell.execute_reply":"2024-04-09T14:35:53.258968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagenet_train, imagenet_test = train_test_split(imagenet_data, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:35:54.053627Z","iopub.execute_input":"2024-04-09T14:35:54.054086Z","iopub.status.idle":"2024-04-09T14:36:42.088910Z","shell.execute_reply.started":"2024-04-09T14:35:54.054052Z","shell.execute_reply":"2024-04-09T14:36:42.088036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TripletImageDataset(Dataset):\n    def __init__(self, dataset, class_to_indices):\n        self.dataset = dataset\n        self.class_to_indices = class_to_indices\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        anchor_img, positive_idx = self.dataset[idx]\n        negative_idx = random.choice([c for c in self.class_to_indices if c != positive_idx])\n        positive_img, _ = self.dataset[positive_idx]\n        negative_img, _ = self.dataset[negative_idx]\n        return anchor_img, positive_img, negative_img\n","metadata":{"id":"CHMYMLKSYXWh","execution":{"iopub.status.busy":"2024-04-09T14:36:42.093852Z","iopub.execute_input":"2024-04-09T14:36:42.094134Z","iopub.status.idle":"2024-04-09T14:36:42.100678Z","shell.execute_reply.started":"2024-04-09T14:36:42.094110Z","shell.execute_reply":"2024-04-09T14:36:42.099731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *SIAMESE NETWORK (PRETRAINED CNN WITH FEED FORWARD)*","metadata":{"id":"zOm1cRQiV8Bx"}},{"cell_type":"code","source":"#Pretrained CNN for feature extraction\nclass pretrained_model(nn.Module):\n\n  def __init__(self, pretrained_CNN, maxpool_ksize, img_trans):\n    super(pretrained_model, self).__init__()\n    self.pre_CNN = pretrained_CNN\n    self.img_trans = img_trans\n    self.mp = nn.MaxPool2d(maxpool_ksize, stride=1)\n    self.fl = nn.Flatten()\n\n  def preprocess(self, x):\n    x = self.img_trans(x)\n    return x\n\n  def forward(self, x):\n    #Extracting Features\n    x = self.pre_CNN(x)\n\n    #2D Maxpooling\n    x = self.mp(x)\n    x = self.fl(x)\n\n    #L2 Normalisation : TBD\n\n    return x\n","metadata":{"id":"dtz9BJy3V4oe","execution":{"iopub.status.busy":"2024-04-09T14:36:42.101684Z","iopub.execute_input":"2024-04-09T14:36:42.102080Z","iopub.status.idle":"2024-04-09T14:36:42.112810Z","shell.execute_reply.started":"2024-04-09T14:36:42.102045Z","shell.execute_reply":"2024-04-09T14:36:42.112059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class feed_forward(nn.Module):\n\n  def __init__(self, input_size):\n    super(feed_forward, self).__init__()\n    self.l1 = nn.Linear(input_size, 2048)\n    self.l2 = nn.Linear(2048, 1024)\n    self.l3 = nn.Linear(1024, 512)\n    self.l4 = nn.Linear(512, 256)\n\n    self.dr1 = nn.Dropout(0.3)\n    self.dr2 = nn.Dropout(0.3)\n    self.dr3 = nn.Dropout(0.25)\n\n    self.relu = nn.ReLU()\n    self.elu = nn.ELU()\n    self.gelu = nn.GELU()\n\n  def forward(self, x):\n    x = self.dr1(self.relu(self.l1(x)))\n    x = self.dr2(self.relu(self.l2(x)))\n    x = self.dr3(self.relu(self.l3(x)))\n    x = self.l4(x)\n    return x","metadata":{"id":"t_8IZP_NKguz","execution":{"iopub.status.busy":"2024-04-09T14:36:49.912623Z","iopub.execute_input":"2024-04-09T14:36:49.913018Z","iopub.status.idle":"2024-04-09T14:36:49.921737Z","shell.execute_reply.started":"2024-04-09T14:36:49.912990Z","shell.execute_reply":"2024-04-09T14:36:49.920724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Siamese(nn.Module):\n  def __init__(self, pretrained_CNN, maxpool_ksize, img_trans):\n    super(Siamese, self).__init__()\n    self.pretrained = pretrained_model(pretrained_CNN, maxpool_ksize, img_trans).to(device)\n    self.pretrained.eval()\n\n    input_shape = (1, 3, 224, 224)\n\n    dummy_input = torch.randn(*input_shape).to(device)\n    output = self.pretrained(dummy_input)\n    feature_size = output.size(1)\n\n    self.ff = feed_forward(feature_size).to(device)\n    print(feature_size)\n\n  def forward(self, x):\n    with torch.no_grad():\n      x = self.pretrained(x)\n    # print(x.shape)\n    x = self.ff(x)\n    return x","metadata":{"id":"r1TE78RYcxLA","execution":{"iopub.status.busy":"2024-04-09T14:36:50.715752Z","iopub.execute_input":"2024-04-09T14:36:50.716658Z","iopub.status.idle":"2024-04-09T14:36:50.723953Z","shell.execute_reply.started":"2024-04-09T14:36:50.716625Z","shell.execute_reply":"2024-04-09T14:36:50.722988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained = models.densenet169(pretrained=True)\nmaxpool_ksize=6\nimg_trans = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"id":"eOe5xmFskRb6","outputId":"7615413d-5d1c-4bfd-c6cd-6a861e83cead","execution":{"iopub.status.busy":"2024-04-09T14:37:09.514761Z","iopub.execute_input":"2024-04-09T14:37:09.515629Z","iopub.status.idle":"2024-04-09T14:37:09.884752Z","shell.execute_reply.started":"2024-04-09T14:37:09.515599Z","shell.execute_reply":"2024-04-09T14:37:09.883726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers = list(pretrained.children())[:-1]\npretrained = torch.nn.Sequential(*layers).to(device)","metadata":{"id":"hhDmqrOjktuG","execution":{"iopub.status.busy":"2024-04-09T14:37:12.457225Z","iopub.execute_input":"2024-04-09T14:37:12.457924Z","iopub.status.idle":"2024-04-09T14:37:12.515282Z","shell.execute_reply.started":"2024-04-09T14:37:12.457890Z","shell.execute_reply":"2024-04-09T14:37:12.514454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model = Siamese(pretrained, maxpool_ksize, img_trans).to(device)","metadata":{"id":"28OLk7I0l4z4","outputId":"4805c98f-8f1e-4aae-afae-97e603301e31","execution":{"iopub.status.busy":"2024-04-09T14:37:15.406204Z","iopub.execute_input":"2024-04-09T14:37:15.406947Z","iopub.status.idle":"2024-04-09T14:37:15.680528Z","shell.execute_reply.started":"2024-04-09T14:37:15.406917Z","shell.execute_reply":"2024-04-09T14:37:15.679566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)","metadata":{"id":"QwA1-4ycnqZz","execution":{"iopub.status.busy":"2024-04-09T14:37:15.902106Z","iopub.execute_input":"2024-04-09T14:37:15.903343Z","iopub.status.idle":"2024-04-09T14:37:15.908992Z","shell.execute_reply.started":"2024-04-09T14:37:15.903302Z","shell.execute_reply":"2024-04-09T14:37:15.907989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 1e-4\nbatch_size = 128\nepochs = 50","metadata":{"id":"fAIS0YdCsABu","execution":{"iopub.status.busy":"2024-04-09T14:37:30.478345Z","iopub.execute_input":"2024-04-09T14:37:30.479277Z","iopub.status.idle":"2024-04-09T14:37:30.483246Z","shell.execute_reply.started":"2024-04-09T14:37:30.479246Z","shell.execute_reply":"2024-04-09T14:37:30.482316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"triplet_dataset = TripletImageDataset(imagenet_train, class_to_indices)\ndata_loader = DataLoader(triplet_dataset, batch_size=batch_size, shuffle=True)","metadata":{"id":"SKsO_t2PsM0l","execution":{"iopub.status.busy":"2024-04-09T14:37:31.070599Z","iopub.execute_input":"2024-04-09T14:37:31.071137Z","iopub.status.idle":"2024-04-09T14:37:31.088036Z","shell.execute_reply.started":"2024-04-09T14:37:31.071102Z","shell.execute_reply":"2024-04-09T14:37:31.087205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.AdamW(siamese_model.parameters(), lr=learning_rate, amsgrad=True)","metadata":{"id":"YvYUBOBhr4Jb","execution":{"iopub.status.busy":"2024-04-09T14:37:31.870156Z","iopub.execute_input":"2024-04-09T14:37:31.870550Z","iopub.status.idle":"2024-04-09T14:37:31.882055Z","shell.execute_reply.started":"2024-04-09T14:37:31.870521Z","shell.execute_reply":"2024-04-09T14:37:31.880905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainLoss = []\n\nfor epoch in range(epochs):\n  trainloss = 0.0\n\n  siamese_model.train()\n  for batch_idx, (anchor_imgs, positive_imgs, negative_imgs) in enumerate(data_loader):\n    optimizer.zero_grad()\n    anchor_out = siamese_model(anchor_imgs.to(device))\n    positive_out = siamese_model(positive_imgs.to(device))\n    negative_out = siamese_model(negative_imgs.to(device))\n    loss = triplet_loss(anchor_out, positive_out, negative_out)\n    trainloss+=loss.item()\n    loss.backward(retain_graph=True)\n    optimizer.step()\n\n  trainLoss.append(trainloss/batch_size)\n  print(\"Epoch: \", epoch+1, \"  TrainLoss: \", trainloss)\n","metadata":{"id":"N1ISp6gWsPh1","outputId":"afdae608-63d6-44a3-d048-a3632af63845","execution":{"iopub.status.busy":"2024-04-09T14:37:32.695764Z","iopub.execute_input":"2024-04-09T14:37:32.696613Z","iopub.status.idle":"2024-04-09T14:49:48.599511Z","shell.execute_reply.started":"2024-04-09T14:37:32.696580Z","shell.execute_reply":"2024-04-09T14:49:48.597087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model.eval()\n\nembeddings = []\nwith torch.no_grad():\n    for barch_idx, (image, label) in enumerate(imagenet_test):\n#         image = img_trans(image).to(device).unsqueeze(0)\n        image = siamese_model(image.to(device).unsqueeze(0)).to('cpu')\n        embeddings.append(image)","metadata":{"id":"45M1J-1P7dum","execution":{"iopub.status.busy":"2024-04-09T14:49:52.135942Z","iopub.execute_input":"2024-04-09T14:49:52.136823Z","iopub.status.idle":"2024-04-09T14:51:35.777448Z","shell.execute_reply.started":"2024-04-09T14:49:52.136789Z","shell.execute_reply":"2024-04-09T14:51:35.776533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_tensor = torch.stack(embeddings).squeeze(1)","metadata":{"id":"rf5HdCex9npS","execution":{"iopub.status.busy":"2024-04-09T14:51:35.779085Z","iopub.execute_input":"2024-04-09T14:51:35.779372Z","iopub.status.idle":"2024-04-09T14:51:35.786121Z","shell.execute_reply.started":"2024-04-09T14:51:35.779347Z","shell.execute_reply":"2024-04-09T14:51:35.785278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_tensor.shape","metadata":{"id":"O6tRZhi6_eBS","outputId":"bc1f641a-92c8-4ccd-ba94-bd77cd0d531b","execution":{"iopub.status.busy":"2024-04-09T14:51:35.787412Z","iopub.execute_input":"2024-04-09T14:51:35.787698Z","iopub.status.idle":"2024-04-09T14:51:35.796718Z","shell.execute_reply.started":"2024-04-09T14:51:35.787675Z","shell.execute_reply":"2024-04-09T14:51:35.795925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors","metadata":{"id":"28LMsYLS9xed","execution":{"iopub.status.busy":"2024-04-09T14:51:35.799180Z","iopub.execute_input":"2024-04-09T14:51:35.799760Z","iopub.status.idle":"2024-04-09T14:51:35.805379Z","shell.execute_reply.started":"2024-04-09T14:51:35.799735Z","shell.execute_reply":"2024-04-09T14:51:35.804614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def knn_grouping(embeddings, k=5):\n    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(embeddings)\n    distances, indices = nbrs.kneighbors(embeddings)\n    return distances, indices","metadata":{"id":"zsJavtTA9yAS","execution":{"iopub.status.busy":"2024-04-09T14:51:35.806759Z","iopub.execute_input":"2024-04-09T14:51:35.807186Z","iopub.status.idle":"2024-04-09T14:51:35.816753Z","shell.execute_reply.started":"2024-04-09T14:51:35.807155Z","shell.execute_reply":"2024-04-09T14:51:35.815972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distances, indices = knn_grouping(embeddings_tensor.numpy(), 11)","metadata":{"id":"1jaYumJF925z","execution":{"iopub.status.busy":"2024-04-09T14:51:35.817914Z","iopub.execute_input":"2024-04-09T14:51:35.818224Z","iopub.status.idle":"2024-04-09T14:51:37.131939Z","shell.execute_reply.started":"2024-04-09T14:51:35.818184Z","shell.execute_reply":"2024-04-09T14:51:37.130505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:51:37.134812Z","iopub.execute_input":"2024-04-09T14:51:37.135267Z","iopub.status.idle":"2024-04-09T14:51:37.143556Z","shell.execute_reply.started":"2024-04-09T14:51:37.135227Z","shell.execute_reply":"2024-04-09T14:51:37.142370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import gridspec\nimport cv2","metadata":{"id":"I7ikWIuPBmSP","execution":{"iopub.status.busy":"2024-04-09T14:51:37.144522Z","iopub.execute_input":"2024-04-09T14:51:37.144873Z","iopub.status.idle":"2024-04-09T14:51:37.157010Z","shell.execute_reply.started":"2024-04-09T14:51:37.144781Z","shell.execute_reply":"2024-04-09T14:51:37.155863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(indx_list, nearest_list, dataset):\n  fig = plt.figure(figsize=(20, 40))\n  gs = gridspec.GridSpec(len(indx_list),len(nearest_list[0])+1)\n  for i in range(len(indx_list)):\n    ax = fig.add_subplot(gs[i,0])\n    image, _ = dataset[indx_list[i]]\n    image = np.transpose(image, (1,2,0))\n    ax.imshow(image)\n    ax.axis('off')\n    for j in range(1, len(nearest_list[indx_list[i]])):\n      ax = fig.add_subplot(gs[i,j+1])\n      img, _ = dataset[nearest_list[indx_list[i]][j]]\n      img = np.transpose(img, (1,2,0))\n      ax.imshow(img)\n      ax.axis('off')","metadata":{"id":"E6rKwBZDB83S","execution":{"iopub.status.busy":"2024-04-09T14:51:37.158362Z","iopub.execute_input":"2024-04-09T14:51:37.158743Z","iopub.status.idle":"2024-04-09T14:51:37.171781Z","shell.execute_reply.started":"2024-04-09T14:51:37.158718Z","shell.execute_reply":"2024-04-09T14:51:37.170671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indx_list = random.sample(range(len(indices)), 10)\nplot_images(indx_list, indices, imagenet_data)","metadata":{"id":"ggBlrWPFDRUB","outputId":"1fea66a6-c7f4-44f9-bf88-456e023caae0","execution":{"iopub.status.busy":"2024-04-09T14:51:37.175497Z","iopub.execute_input":"2024-04-09T14:51:37.175926Z","iopub.status.idle":"2024-04-09T14:51:42.363350Z","shell.execute_reply.started":"2024-04-09T14:51:37.175888Z","shell.execute_reply":"2024-04-09T14:51:42.362124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *AUTOENCODER*","metadata":{"id":"GLt4yMpnYPCJ"}},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n\n  def __init__(self, input_shape):\n    super(AutoEncoder, self).__init__()\n\n    #Encoder Layers\n    self.l1 = nn.Linear(input_shape, 2048)\n    self.l2 = nn.Linear(2048, 1024)\n    self.l3 = nn.Linear(1024, 512)\n    self.l4 = nn.Linear(512, 256)\n    self.dr1 = nn.Dropout(0.3)\n    self.dr2 = nn.Dropout(0.3)\n    self.dr3 = nn.Dropout(0.25)\n    self.dr4 = nn.Dropout(0.2)\n\n    #Decoder Layers\n    self.l5 = nn.Linear(256, 512)\n    self.l6 = nn.Linear(512, 1024)\n    self.l7 = nn.Linear(1024, 2048)\n    self.l8 = nn.Linear(2048, input_shape)\n    self.dr5 = nn.Dropout(0.2)\n    self.dr6 = nn.Dropout(0.25)\n    self.dr7 = nn.Dropout(0.3)\n\n    #Activation Functions\n    self.relu = nn.ReLU()\n    self.elu = nn.ELU()\n    self.gelu = nn.GELU()\n\n  #Encoder Forward Pass\n  def encoder(self, x):\n    x = self.dr1(self.relu(self.l1(x)))\n    x = self.dr2(self.relu(self.l2(x)))\n    x = self.dr3(self.relu(self.l3(x)))\n    x = self.dr4(self.relu(self.l4(x)))\n\n  #Decoder Forward Pass\n  def decoder(self, x):\n    x = self.dr5(self.relu(self.l5(x)))\n    x = self.dr6(self.relu(self.l6(x)))\n    x = self.dr7(self.relu(self.l7(x)))\n    x = self.l8(x)\n    return x\n\n  #Forward Pass\n  def forward(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n\n    #Return decoder output as well as bottlneck output\n    return decoded, encoded\n\n","metadata":{"id":"lNeqlfcRXwVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class LinearAutoencoder(nn.Module):\n#     def __init__(self, input_dim, encoding_dim):        super(LinearAutoencoder, self).__init__()\n#         self.encoder = nn.Linear(input_dim, encoding_dim)\n#         self.decoder = nn.Linear(encoding_dim, input_dim)\n\n#     def forward(self, x):\n#         x = self.encoder(x)\n#         x = self.decoder(x)\n#         return x\n","metadata":{"id":"lGVYtaBuddZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import requests\n# from zipfile import ZipFile\n\n# dataset_url = \"http://aws-proserve-data-science.s3.amazonaws.com/geological_similarity.zip\"\n# filePath = './data_repository/geological_similarity.zip'\n# data_directory = './data_repository'\n\n# if not os.path.exists(data_directory):\n#     try:\n#         os.makedirs(data_directory)\n#         print(data_directory,\" created successfully.\")\n#     except:\n#         print(\"Unable to create directory at \",data_directory,\" Please create \",data_directory,\" manually. Then run this file again.\")\n\n# if os.path.exists(filePath):\n#     os.remove(filePath)\n# else:\n#     print(\"Have to download dataset.\")\n\n\n# r = requests.get(dataset_url, stream = True)\n# print('Started downloading dataset...')\n# with open(filePath, \"wb\") as data:\n#     for chunk in r.iter_content(chunk_size=1024):\n#         # writing one chunk at a time to data file\n\n#         if chunk:\n#             print('...',end = ''),\n#             data.write(chunk)\n# print('Download finished.')\n# print('Unzipping File...')\n# zf = ZipFile(filePath, 'r')\n# zf.extractall('./data_repository/')\n# zf.close()\n# print('Successfully unzipped file. Ready to run model...')","metadata":{"id":"vHROCNP5LpbT","outputId":"4b83aa46-9ccd-4ea5-d37b-fed9a44878ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom matplotlib.image import imread\nimport numpy as np\n\n\nclass PreProcessing:\n\n    images_train = np.array([])\n    images_test = np.array([])\n    labels_train = np.array([])\n    labels_test = np.array([])\n    unique_train_label = np.array([])\n    map_train_label_indices = dict()\n\n    def __init__(self,data_src):\n        self.data_src = data_src\n        print(\"Loading Geological Similarity Dataset...\")\n        self.images_train, self.images_test, self.labels_train, self.labels_test = self.preprocessing(0.9)\n        self.unique_train_label = np.unique(self.labels_train)\n        self.map_train_label_indices = {label: np.flatnonzero(self.labels_train == label) for label in\n                                        self.unique_train_label}\n        print('Preprocessing Done. Summary:')\n        print(\"Images train :\", self.images_train.shape)\n        print(\"Labels train :\", self.labels_train.shape)\n        print(\"Images test  :\", self.images_test.shape)\n        print(\"Labels test  :\", self.labels_test.shape)\n        print(\"Unique label :\", self.unique_train_label)\n\n    def normalize(self,x):\n        min_val = np.min(x)\n        max_val = np.max(x)\n        x = (x - min_val) / (max_val - min_val)\n        return x\n\n    def read_dataset(self):\n        X = []\n        y = []\n        for directory in os.listdir(self.data_src):\n            # print(directory)\n            try:\n                i = 0\n                for pic in os.listdir(os.path.join(self.data_src, directory)):\n                    if i==100:\n                      break\n                    img = imread(os.path.join(self.data_src, directory, pic))\n                    X.append(np.squeeze(np.asarray(img)))\n                    y.append(directory)\n                    i+=1\n            except Exception as e:\n                print('Failed to read images from Directory: ', directory)\n                print('Exception Message: ', e)\n        print('Dataset loaded successfully.')\n        return X,y\n\n    def preprocessing(self,train_test_ratio):\n        X, y = self.read_dataset()\n        labels = list(set(y))\n        label_dict = dict(zip(labels, range(len(labels))))\n        Y = np.asarray([label_dict[label] for label in y])\n        X = [self.normalize(x) for x in X]                                  # normalize images\n\n        shuffle_indices = np.random.permutation(np.arange(len(y)))\n        x_shuffled = []\n        y_shuffled = []\n        for index in shuffle_indices:\n            x_shuffled.append(X[index])\n            y_shuffled.append(Y[index])\n\n        size_of_dataset = len(x_shuffled)\n        n_train = int(np.ceil(size_of_dataset * train_test_ratio))\n        print(len(x_shuffled), len(y_s))\n        return np.asarray(x_shuffled[0:n_train]), np.asarray(x_shuffled[n_train + 1:size_of_dataset]), np.asarray(\n            y_shuffled[0:n_train]), np.asarray(y_shuffled[\n                                               n_train + 1:size_of_dataset])\n\n\n    def get_triplets(self):\n        label_l, label_r = np.random.choice(self.unique_train_label, 2, replace=False)\n        a, p = np.random.choice(self.map_train_label_indices[label_l],2, replace=False)\n        n = np.random.choice(self.map_train_label_indices[label_r])\n        return a, p, n\n\n    def get_triplets_batch(self,n):\n        idxs_a, idxs_p, idxs_n = [], [], []\n        for _ in range(n):\n            a, p, n = self.get_triplets()\n            idxs_a.append(a)\n            idxs_p.append(p)\n            idxs_n.append(n)\n        return self.images_train[idxs_a,:], self.images_train[idxs_p, :], self.images_train[idxs_n, :]","metadata":{"id":"BlE9Md1yMjbD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = PreProcessing('/content/ImageNette/imagenette2/train')","metadata":{"id":"eJqHg3oSQAGH","outputId":"66430180-6247-44ba-f239-bcd0e739d212"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7JEFo8BNQWiU"},"execution_count":null,"outputs":[]}]}