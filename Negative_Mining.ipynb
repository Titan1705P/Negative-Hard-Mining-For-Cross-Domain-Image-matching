{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AQSHPH9P0BNx"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "mean, std = 0.1307, 0.3081\n",
    "\n",
    "train_dataset = MNIST('../data/MNIST', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((mean,), (std,))\n",
    "                             ]))\n",
    "test_dataset = MNIST('../data/MNIST', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean,), (std,))\n",
    "                            ]))\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scl/anaconda3/envs/rl/lib/python3.7/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from trainer import fit\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "VIS_df = pd.read_csv('VIS_features.csv')\n",
    "NIR_df = pd.read_csv('NIR_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIS_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "NIR_df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,Subset\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):        \n",
    "\n",
    "    def __init__(self, dataset_file, transform=None):\n",
    "        super().__init__()\n",
    "        dataset = dataset_file\n",
    "        self.labels_frame = np.array(pd.DataFrame(dataset['class']), dtype=np.float32).squeeze(1)\n",
    "        self.features_frame = np.array(dataset.drop(['class'], axis=1), dtype=np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features_frame[idx]\n",
    "        label = self.labels_frame[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "VIS_train_dataset, VIS_test_dataset = train_test_split(VIS_df, test_size=test_size)\n",
    "NIR_train_dataset, NIR_test_dataset = train_test_split(NIR_df, test_size=test_size)\n",
    "VIS_train_dataset = CustomDataset(VIS_train_dataset)\n",
    "VIS_test_dataset = CustomDataset(VIS_test_dataset)\n",
    "NIR_train_dataset = CustomDataset(NIR_train_dataset)\n",
    "NIR_test_dataset = CustomDataset(NIR_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "# # Create data loaders\n",
    "# VIS_train_loader = DataLoader(VIS_train_dataset, batch_size=64)\n",
    "# VIS_test_loader = DataLoader(VIS_test_dataset, batch_size=64)\n",
    "# NIR_train_loader = DataLoader(NIR_train_dataset, batch_size=64)\n",
    "# NIR_test_loader = DataLoader(NIR_test_dataset, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n",
    "    \n",
    "#print(type(VIS_dataset))\n",
    "VIS_train_batch_sampler = BalancedBatchSampler(torch.tensor(VIS_train_dataset.labels_frame), n_classes=16, n_samples=4)\n",
    "VIS_test_batch_sampler = BalancedBatchSampler(torch.tensor(VIS_test_dataset.labels_frame), n_classes=16, n_samples=4)\n",
    "NIR_train_batch_sampler = BalancedBatchSampler(torch.tensor(NIR_train_dataset.labels_frame), n_classes=16, n_samples=4)\n",
    "NIR_test_batch_sampler = BalancedBatchSampler(torch.tensor(NIR_test_dataset.labels_frame), n_classes=16, n_samples=4)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "VIS_train_loader = torch.utils.data.DataLoader(VIS_train_dataset, batch_sampler=VIS_train_batch_sampler, **kwargs)\n",
    "VIS_test_loader = torch.utils.data.DataLoader(VIS_test_dataset, batch_sampler=VIS_test_batch_sampler, **kwargs)\n",
    "\n",
    "NIR_train_loader = torch.utils.data.DataLoader(NIR_train_dataset, batch_sampler=NIR_train_batch_sampler, **kwargs)\n",
    "NIR_test_loader = torch.utils.data.DataLoader(NIR_test_dataset, batch_sampler=NIR_test_batch_sampler, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NIR_train_batch_sampler = BalancedBatchSampler(NIR_train_dataset.labels_frame, n_classes=16, n_samples=4)\n",
    "# NIR_test_batch_sampler = BalancedBatchSampler(NIR_test_dataset.labels_frame, n_classes=16, n_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(512, 256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(256, 128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(128, 64),\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Triplet Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Online Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
    "    triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, triplet_selector):\n",
    "        super(OnlineTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.triplet_selector = triplet_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "\n",
    "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            triplets = triplets.cuda()\n",
    "\n",
    "        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n",
    "        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
    "\n",
    "        return losses.mean(), len(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard Mining Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(vectors):\n",
    "    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n",
    "        dim=1).view(-1, 1)\n",
    "    return distance_matrix\n",
    "\n",
    "class TripletSelector:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AllTripletSelector(TripletSelector):\n",
    "    def __init__(self):\n",
    "        super(AllTripletSelector, self).__init__()\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "\n",
    "            # Add all negatives for all positive pairs\n",
    "            temp_triplets = [[anchor_positive[0], anchor_positive[1], neg_ind] for anchor_positive in anchor_positives\n",
    "                             for neg_ind in negative_indices]\n",
    "            triplets += temp_triplets\n",
    "\n",
    "        return torch.LongTensor(np.array(triplets))\n",
    "\n",
    "\n",
    "def hardest_negative(loss_values):\n",
    "    hard_negative = np.argmax(loss_values)\n",
    "    return hard_negative if loss_values[hard_negative] > 0 else None\n",
    "\n",
    "\n",
    "def random_hard_negative(loss_values):\n",
    "    hard_negatives = np.where(loss_values > 0)[0]\n",
    "    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n",
    "\n",
    "\n",
    "def semihard_negative(loss_values, margin):\n",
    "    semihard_negatives = np.where(np.logical_and(loss_values < margin, loss_values > 0))[0]\n",
    "    return np.random.choice(semihard_negatives) if len(semihard_negatives) > 0 else None\n",
    "\n",
    "\n",
    "class FunctionNegativeTripletSelector(TripletSelector):\n",
    "    def __init__(self, margin, negative_selection_fn, cpu=True):\n",
    "        super(FunctionNegativeTripletSelector, self).__init__()\n",
    "        self.cpu = cpu\n",
    "        self.margin = margin\n",
    "        self.negative_selection_fn = negative_selection_fn\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        if self.cpu:\n",
    "            embeddings = embeddings.cpu()\n",
    "        distance_matrix = pdist(embeddings)\n",
    "        distance_matrix = distance_matrix.cpu()\n",
    "\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "            anchor_positives = np.array(anchor_positives)\n",
    "\n",
    "            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n",
    "            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n",
    "                loss_values = ap_distance - distance_matrix[torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n",
    "                loss_values = loss_values.data.cpu().numpy()\n",
    "                hard_negative = self.negative_selection_fn(loss_values)\n",
    "                if hard_negative is not None:\n",
    "                    hard_negative = negative_indices[hard_negative]\n",
    "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
    "\n",
    "        if len(triplets) == 0:\n",
    "            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n",
    "\n",
    "        triplets = np.array(triplets)\n",
    "\n",
    "        return torch.LongTensor(triplets)\n",
    "\n",
    "\n",
    "def HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n",
    "                                                                                 negative_selection_fn=hardest_negative,\n",
    "                                                                                 cpu=cpu)\n",
    "\n",
    "\n",
    "def RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n",
    "                                                                                negative_selection_fn=random_hard_negative,\n",
    "                                                                                cpu=cpu)\n",
    "\n",
    "\n",
    "def SemihardNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n",
    "                                                                                  negative_selection_fn=lambda x: semihard_negative(x, margin),\n",
    "                                                                                  cpu=cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavvajreshwari/Library/Python/3.9/lib/python/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/1536 (0%)]\tLoss: 0.999194\tAverage nonzero triplets: 96.0\n",
      "Epoch: 1/20. Train set: Average loss: 0.8928\tAverage nonzero triplets: 95.6086956521739\n",
      "Epoch: 1/20. Validation set: Average loss: 0.8422\tAverage nonzero triplets: 22.4\n",
      "Train: [0/1536 (0%)]\tLoss: 0.783064\tAverage nonzero triplets: 96.0\n",
      "Epoch: 2/20. Train set: Average loss: 1.0016\tAverage nonzero triplets: 90.8695652173913\n",
      "Epoch: 2/20. Validation set: Average loss: 0.7712\tAverage nonzero triplets: 19.4\n",
      "Train: [0/1536 (0%)]\tLoss: 0.684489\tAverage nonzero triplets: 83.0\n",
      "Epoch: 3/20. Train set: Average loss: 0.9046\tAverage nonzero triplets: 87.0\n",
      "Epoch: 3/20. Validation set: Average loss: 0.7707\tAverage nonzero triplets: 16.4\n",
      "Train: [0/1536 (0%)]\tLoss: 1.355471\tAverage nonzero triplets: 72.0\n",
      "Epoch: 4/20. Train set: Average loss: 1.0144\tAverage nonzero triplets: 83.26086956521739\n",
      "Epoch: 4/20. Validation set: Average loss: 0.5914\tAverage nonzero triplets: 19.4\n",
      "Train: [0/1536 (0%)]\tLoss: 0.653968\tAverage nonzero triplets: 80.0\n",
      "Epoch: 5/20. Train set: Average loss: 0.9990\tAverage nonzero triplets: 79.30434782608695\n",
      "Epoch: 5/20. Validation set: Average loss: 0.6605\tAverage nonzero triplets: 10.8\n",
      "Train: [0/1536 (0%)]\tLoss: 0.761207\tAverage nonzero triplets: 89.0\n",
      "Epoch: 6/20. Train set: Average loss: 0.9605\tAverage nonzero triplets: 80.26086956521739\n",
      "Epoch: 6/20. Validation set: Average loss: 1.0014\tAverage nonzero triplets: 15.0\n",
      "Train: [0/1536 (0%)]\tLoss: 1.017850\tAverage nonzero triplets: 83.0\n",
      "Epoch: 7/20. Train set: Average loss: 0.9764\tAverage nonzero triplets: 69.0\n",
      "Epoch: 7/20. Validation set: Average loss: 1.0946\tAverage nonzero triplets: 6.8\n",
      "Train: [0/1536 (0%)]\tLoss: 0.790486\tAverage nonzero triplets: 75.0\n",
      "Epoch: 8/20. Train set: Average loss: 0.9382\tAverage nonzero triplets: 67.6086956521739\n",
      "Epoch: 8/20. Validation set: Average loss: 0.8111\tAverage nonzero triplets: 13.0\n",
      "Train: [0/1536 (0%)]\tLoss: 0.772420\tAverage nonzero triplets: 50.0\n",
      "Epoch: 9/20. Train set: Average loss: 0.8756\tAverage nonzero triplets: 59.69565217391305\n",
      "Epoch: 9/20. Validation set: Average loss: 1.4396\tAverage nonzero triplets: 12.2\n",
      "Train: [0/1536 (0%)]\tLoss: 0.936301\tAverage nonzero triplets: 67.0\n",
      "Epoch: 10/20. Train set: Average loss: 0.9892\tAverage nonzero triplets: 62.34782608695652\n",
      "Epoch: 10/20. Validation set: Average loss: 1.2616\tAverage nonzero triplets: 13.8\n",
      "Train: [0/1536 (0%)]\tLoss: 1.341316\tAverage nonzero triplets: 81.0\n",
      "Epoch: 11/20. Train set: Average loss: 0.9606\tAverage nonzero triplets: 61.95652173913044\n",
      "Epoch: 11/20. Validation set: Average loss: 0.8540\tAverage nonzero triplets: 15.6\n",
      "Train: [0/1536 (0%)]\tLoss: 1.003301\tAverage nonzero triplets: 70.0\n",
      "Epoch: 12/20. Train set: Average loss: 0.8924\tAverage nonzero triplets: 64.34782608695652\n",
      "Epoch: 12/20. Validation set: Average loss: 0.8960\tAverage nonzero triplets: 14.0\n",
      "Train: [0/1536 (0%)]\tLoss: 1.103922\tAverage nonzero triplets: 47.0\n",
      "Epoch: 13/20. Train set: Average loss: 0.9576\tAverage nonzero triplets: 61.65217391304348\n",
      "Epoch: 13/20. Validation set: Average loss: 0.7700\tAverage nonzero triplets: 10.8\n",
      "Train: [0/1536 (0%)]\tLoss: 0.823496\tAverage nonzero triplets: 67.0\n",
      "Epoch: 14/20. Train set: Average loss: 0.9489\tAverage nonzero triplets: 61.43478260869565\n",
      "Epoch: 14/20. Validation set: Average loss: 1.2463\tAverage nonzero triplets: 10.2\n",
      "Train: [0/1536 (0%)]\tLoss: 1.715856\tAverage nonzero triplets: 55.0\n",
      "Epoch: 15/20. Train set: Average loss: 0.9876\tAverage nonzero triplets: 64.78260869565217\n",
      "Epoch: 15/20. Validation set: Average loss: 1.3651\tAverage nonzero triplets: 15.2\n",
      "Train: [0/1536 (0%)]\tLoss: 0.829958\tAverage nonzero triplets: 62.0\n",
      "Epoch: 16/20. Train set: Average loss: 0.9169\tAverage nonzero triplets: 61.52173913043478\n",
      "Epoch: 16/20. Validation set: Average loss: 1.6804\tAverage nonzero triplets: 6.6\n",
      "Train: [0/1536 (0%)]\tLoss: 1.036264\tAverage nonzero triplets: 53.0\n",
      "Epoch: 17/20. Train set: Average loss: 0.9035\tAverage nonzero triplets: 63.47826086956522\n",
      "Epoch: 17/20. Validation set: Average loss: 0.9678\tAverage nonzero triplets: 10.8\n",
      "Train: [0/1536 (0%)]\tLoss: 1.651949\tAverage nonzero triplets: 63.0\n",
      "Epoch: 18/20. Train set: Average loss: 0.9727\tAverage nonzero triplets: 61.52173913043478\n",
      "Epoch: 18/20. Validation set: Average loss: 0.8820\tAverage nonzero triplets: 9.0\n",
      "Train: [0/1536 (0%)]\tLoss: 0.977635\tAverage nonzero triplets: 53.0\n",
      "Epoch: 19/20. Train set: Average loss: 1.0038\tAverage nonzero triplets: 63.78260869565217\n",
      "Epoch: 19/20. Validation set: Average loss: 1.1243\tAverage nonzero triplets: 9.0\n",
      "Train: [0/1536 (0%)]\tLoss: 1.138216\tAverage nonzero triplets: 49.0\n",
      "Epoch: 20/20. Train set: Average loss: 0.9114\tAverage nonzero triplets: 59.78260869565217\n",
      "Epoch: 20/20. Validation set: Average loss: 0.8652\tAverage nonzero triplets: 12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from metrics import AverageNonzeroTripletsMetric\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineTripletLoss(margin,RandomNegativeTripletSelector(margin))\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50\n",
    "\n",
    "fit(NIR_train_loader, NIR_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNetmnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNetmnist, self).__init__()\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 2)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.convnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavvajreshwari/Library/Python/3.9/lib/python/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/Users/pranavvajreshwari/Library/Python/3.9/lib/python/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "\n",
    "from metrics import AverageNonzeroTripletsMetric\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNetmnist()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/60000 (0%)]\tLoss: 0.496676\tAverage nonzero triplets: 813.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.497699\tAverage nonzero triplets: 635.1372549019608\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.493066\tAverage nonzero triplets: 553.1485148514852\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.494565\tAverage nonzero triplets: 507.9933774834437\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.494200\tAverage nonzero triplets: 457.29353233830847\n",
      "Epoch: 1/20. Train set: Average loss: 0.4940\tAverage nonzero triplets: 429.3389121338912\n",
      "Epoch: 1/20. Validation set: Average loss: 0.4844\tAverage nonzero triplets: 200.82051282051282\n",
      "Train: [0/60000 (0%)]\tLoss: 0.481051\tAverage nonzero triplets: 176.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.492920\tAverage nonzero triplets: 201.7843137254902\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.499208\tAverage nonzero triplets: 176.69306930693068\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.501378\tAverage nonzero triplets: 154.2913907284768\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.506688\tAverage nonzero triplets: 131.0547263681592\n",
      "Epoch: 2/20. Train set: Average loss: 0.5000\tAverage nonzero triplets: 118.14644351464435\n",
      "Epoch: 2/20. Validation set: Average loss: 0.4895\tAverage nonzero triplets: 49.666666666666664\n",
      "Train: [0/60000 (0%)]\tLoss: 0.528281\tAverage nonzero triplets: 48.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.496102\tAverage nonzero triplets: 35.84313725490196\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.496499\tAverage nonzero triplets: 30.623762376237625\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.507267\tAverage nonzero triplets: 28.039735099337747\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.503249\tAverage nonzero triplets: 24.223880597014926\n",
      "Epoch: 3/20. Train set: Average loss: 0.5012\tAverage nonzero triplets: 22.556485355648537\n",
      "Epoch: 3/20. Validation set: Average loss: 0.4663\tAverage nonzero triplets: 9.538461538461538\n",
      "Train: [0/60000 (0%)]\tLoss: 0.684254\tAverage nonzero triplets: 7.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.498341\tAverage nonzero triplets: 7.705882352941177\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.516173\tAverage nonzero triplets: 6.6138613861386135\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.528528\tAverage nonzero triplets: 7.662251655629139\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.424181\tAverage nonzero triplets: 6.781094527363184\n",
      "Epoch: 4/20. Train set: Average loss: 0.4900\tAverage nonzero triplets: 6.2887029288702925\n",
      "Epoch: 4/20. Validation set: Average loss: 290.7225\tAverage nonzero triplets: 1.794871794871795\n",
      "Train: [0/60000 (0%)]\tLoss: 0.993164\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.420989\tAverage nonzero triplets: 2.0\n",
      "Train: [25000/60000 (42%)]\tLoss: 944.140479\tAverage nonzero triplets: 2.128712871287129\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.397198\tAverage nonzero triplets: 1.9403973509933774\n",
      "Train: [50000/60000 (83%)]\tLoss: 3962.654394\tAverage nonzero triplets: 2.1691542288557213\n",
      "Epoch: 5/20. Train set: Average loss: 1026.7666\tAverage nonzero triplets: 2.1757322175732217\n",
      "Epoch: 5/20. Validation set: Average loss: 0.3323\tAverage nonzero triplets: 1.4615384615384615\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.418389\tAverage nonzero triplets: 2.5294117647058822\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.367545\tAverage nonzero triplets: 2.1782178217821784\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.383592\tAverage nonzero triplets: 1.9933774834437086\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.242279\tAverage nonzero triplets: 1.8258706467661692\n",
      "Epoch: 6/20. Train set: Average loss: 0.3470\tAverage nonzero triplets: 1.7573221757322175\n",
      "Epoch: 6/20. Validation set: Average loss: 0.3155\tAverage nonzero triplets: 1.3076923076923077\n",
      "Train: [0/60000 (0%)]\tLoss: 0.205811\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.338648\tAverage nonzero triplets: 1.4901960784313726\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.214614\tAverage nonzero triplets: 1.4455445544554455\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.246513\tAverage nonzero triplets: 1.4172185430463575\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.285128\tAverage nonzero triplets: 1.3532338308457712\n",
      "Epoch: 7/20. Train set: Average loss: 0.2713\tAverage nonzero triplets: 1.3179916317991631\n",
      "Epoch: 7/20. Validation set: Average loss: 0.1514\tAverage nonzero triplets: 1.205128205128205\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 116.517377\tAverage nonzero triplets: 1.2745098039215685\n",
      "Train: [25000/60000 (42%)]\tLoss: 551.398018\tAverage nonzero triplets: 1.2376237623762376\n",
      "Train: [37500/60000 (62%)]\tLoss: 688.788809\tAverage nonzero triplets: 1.205298013245033\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.253561\tAverage nonzero triplets: 1.199004975124378\n",
      "Epoch: 8/20. Train set: Average loss: 283.9222\tAverage nonzero triplets: 1.200836820083682\n",
      "Epoch: 8/20. Validation set: Average loss: 0.3217\tAverage nonzero triplets: 1.2307692307692308\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.201700\tAverage nonzero triplets: 1.1176470588235294\n",
      "Train: [25000/60000 (42%)]\tLoss: 76.489765\tAverage nonzero triplets: 1.108910891089109\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.097998\tAverage nonzero triplets: 1.099337748344371\n",
      "Train: [50000/60000 (83%)]\tLoss: 25.196373\tAverage nonzero triplets: 1.1293532338308458\n",
      "Epoch: 9/20. Train set: Average loss: 21.3796\tAverage nonzero triplets: 1.1380753138075315\n",
      "Epoch: 9/20. Validation set: Average loss: 84.0009\tAverage nonzero triplets: 1.205128205128205\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.236072\tAverage nonzero triplets: 1.1372549019607843\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.266071\tAverage nonzero triplets: 1.1584158415841583\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.201499\tAverage nonzero triplets: 1.1589403973509933\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.242732\tAverage nonzero triplets: 1.1592039800995024\n",
      "Epoch: 10/20. Train set: Average loss: 0.2288\tAverage nonzero triplets: 1.1506276150627615\n",
      "Epoch: 10/20. Validation set: Average loss: 0.1335\tAverage nonzero triplets: 1.0512820512820513\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.201353\tAverage nonzero triplets: 1.1372549019607843\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.184631\tAverage nonzero triplets: 1.118811881188119\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.232558\tAverage nonzero triplets: 1.1655629139072847\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.188781\tAverage nonzero triplets: 1.164179104477612\n",
      "Epoch: 11/20. Train set: Average loss: 18.9489\tAverage nonzero triplets: 1.1506276150627615\n",
      "Epoch: 11/20. Validation set: Average loss: 0.2417\tAverage nonzero triplets: 1.3333333333333333\n",
      "Train: [0/60000 (0%)]\tLoss: 0.517090\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 54.956380\tAverage nonzero triplets: 1.1176470588235294\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.254675\tAverage nonzero triplets: 1.118811881188119\n",
      "Train: [37500/60000 (62%)]\tLoss: 155.273021\tAverage nonzero triplets: 1.086092715231788\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.190715\tAverage nonzero triplets: 1.099502487562189\n",
      "Epoch: 12/20. Train set: Average loss: 44.1052\tAverage nonzero triplets: 1.104602510460251\n",
      "Epoch: 12/20. Validation set: Average loss: 0.1211\tAverage nonzero triplets: 1.1282051282051282\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.131374\tAverage nonzero triplets: 1.0784313725490196\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.219760\tAverage nonzero triplets: 1.1683168316831682\n",
      "Train: [37500/60000 (62%)]\tLoss: 243.824648\tAverage nonzero triplets: 1.1390728476821192\n",
      "Train: [50000/60000 (83%)]\tLoss: 15.883597\tAverage nonzero triplets: 1.1293532338308458\n",
      "Epoch: 13/20. Train set: Average loss: 54.4421\tAverage nonzero triplets: 1.1255230125523012\n",
      "Epoch: 13/20. Validation set: Average loss: 0.1800\tAverage nonzero triplets: 1.1538461538461537\n",
      "Train: [0/60000 (0%)]\tLoss: 0.608887\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 162.145254\tAverage nonzero triplets: 1.2352941176470589\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.194789\tAverage nonzero triplets: 1.1386138613861385\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.224622\tAverage nonzero triplets: 1.1258278145695364\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.166730\tAverage nonzero triplets: 1.109452736318408\n",
      "Epoch: 14/20. Train set: Average loss: 34.0840\tAverage nonzero triplets: 1.112970711297071\n",
      "Epoch: 14/20. Validation set: Average loss: 0.2320\tAverage nonzero triplets: 1.1538461538461537\n",
      "Train: [0/60000 (0%)]\tLoss: 0.120117\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.232881\tAverage nonzero triplets: 1.2156862745098038\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.211811\tAverage nonzero triplets: 1.198019801980198\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.223385\tAverage nonzero triplets: 1.1788079470198676\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.203115\tAverage nonzero triplets: 1.1492537313432836\n",
      "Epoch: 15/20. Train set: Average loss: 0.2132\tAverage nonzero triplets: 1.1338912133891212\n",
      "Epoch: 15/20. Validation set: Average loss: 0.2418\tAverage nonzero triplets: 1.1538461538461537\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.220321\tAverage nonzero triplets: 1.1372549019607843\n",
      "Train: [25000/60000 (42%)]\tLoss: 65.862310\tAverage nonzero triplets: 1.1485148514851484\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.186095\tAverage nonzero triplets: 1.1589403973509933\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.227332\tAverage nonzero triplets: 1.1890547263681592\n",
      "Epoch: 16/20. Train set: Average loss: 13.9382\tAverage nonzero triplets: 1.1631799163179917\n",
      "Epoch: 16/20. Validation set: Average loss: 0.1967\tAverage nonzero triplets: 1.0512820512820513\n",
      "Train: [0/60000 (0%)]\tLoss: 0.493408\tAverage nonzero triplets: 2.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.205224\tAverage nonzero triplets: 1.1764705882352942\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.282035\tAverage nonzero triplets: 1.1386138613861385\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.231012\tAverage nonzero triplets: 1.1324503311258278\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.204995\tAverage nonzero triplets: 1.1492537313432836\n",
      "Epoch: 17/20. Train set: Average loss: 0.2213\tAverage nonzero triplets: 1.1338912133891212\n",
      "Epoch: 17/20. Validation set: Average loss: 0.2174\tAverage nonzero triplets: 1.1538461538461537\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.139212\tAverage nonzero triplets: 1.0588235294117647\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.251330\tAverage nonzero triplets: 1.0891089108910892\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.233739\tAverage nonzero triplets: 1.1125827814569536\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.140761\tAverage nonzero triplets: 1.1343283582089552\n",
      "Epoch: 18/20. Train set: Average loss: 0.1802\tAverage nonzero triplets: 1.1297071129707112\n",
      "Epoch: 18/20. Validation set: Average loss: 62.2132\tAverage nonzero triplets: 1.1025641025641026\n",
      "Train: [0/60000 (0%)]\tLoss: 0.832031\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 0.220168\tAverage nonzero triplets: 1.1176470588235294\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.269402\tAverage nonzero triplets: 1.1584158415841583\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.214739\tAverage nonzero triplets: 1.1324503311258278\n",
      "Train: [50000/60000 (83%)]\tLoss: 164.708514\tAverage nonzero triplets: 1.154228855721393\n",
      "Epoch: 19/20. Train set: Average loss: 34.6248\tAverage nonzero triplets: 1.1422594142259415\n",
      "Epoch: 19/20. Validation set: Average loss: 0.1293\tAverage nonzero triplets: 1.0769230769230769\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\tAverage nonzero triplets: 1.0\n",
      "Train: [12500/60000 (21%)]\tLoss: 149.939752\tAverage nonzero triplets: 1.0588235294117647\n",
      "Train: [25000/60000 (42%)]\tLoss: 0.187833\tAverage nonzero triplets: 1.1485148514851484\n",
      "Train: [37500/60000 (62%)]\tLoss: 0.153067\tAverage nonzero triplets: 1.099337748344371\n",
      "Train: [50000/60000 (83%)]\tLoss: 0.240300\tAverage nonzero triplets: 1.1293532338308458\n",
      "Epoch: 20/20. Train set: Average loss: 31.5197\tAverage nonzero triplets: 1.1380753138075315\n",
      "Epoch: 20/20. Validation set: Average loss: 0.1439\tAverage nonzero triplets: 1.1025641025641026\n"
     ]
    }
   ],
   "source": [
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "[[1.]]\n",
      "4 9\n",
      "Genuine pair\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "def get_embeddings(model, image_loader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in image_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            embeddings.append(outputs.cpu().numpy())\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def calculate_cosine_similarity(embeddings1, embeddings2):\n",
    "    return cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "# Define a function to check if similarity is within a threshold\n",
    "def check_similarity(similarity, threshold):\n",
    "    return similarity >= threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_embeddings = get_embeddings(model, test_loader)\n",
    "\n",
    "# Take two images from test dataset\n",
    "image1, label1 = test_dataset[67]\n",
    "image2, label2 = test_dataset[575]\n",
    "\n",
    "print(image1.shape)\n",
    "\n",
    "\n",
    "# Get embeddings for the two images\n",
    "image1_embedding = model((image1.to(device).unsqueeze(0))).cpu().detach().numpy()\n",
    "image2_embedding = model((image2.to(device).unsqueeze(0))).cpu().detach().numpy()\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = calculate_cosine_similarity(image1_embedding, image2_embedding)\n",
    "\n",
    "# Define threshold\n",
    "print(similarity)\n",
    "print(label1,label2)\n",
    "# Check if similarity is within threshold\n",
    "if check_similarity(similarity, threshold=0.1):\n",
    "    print(\"Genuine pair\")\n",
    "else:\n",
    "    print(\"Fraud pair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Experiments_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
